{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas   # ensures pandas is installed properly, local env issues ignore or delete as needed\n",
    "import pandas as pd\n",
    "\n",
    "class csv_read_in_functions:\n",
    "    csv_path = \"../dataset/hirevue-answer-sheet.csv\"\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        # loads the csv when the class is instantiated \n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # verify columns exist, behavioral does not matter just ensuring it exists \n",
    "        required_columns = {\"BEHAVIORAL_QUESTIONS\",\"SAMPLE_POSITIVE_ANSWERS\",\"POSITIVE_SENTIMENT\",\"SAMPLE_NEGATIVE_ANSWERS\",\"NEGATIVE_SENTIMENT\"}\n",
    "        if not required_columns.issubset(self.df.columns):\n",
    "            raise ValueError(f\"CSV file must contain the columns: {required_columns}\")\n",
    "        self.positive_sentences = []  # store positive tuples: (positive_answer, positive_sentiment_expected)\n",
    "        self.negative_sentences = []  # store negative tuples: (negative_answer, negative_sentiment_expected)\n",
    "\n",
    "    def grab_sentences_and_sentiment(self):\n",
    "        \"\"\"\n",
    "        gets each row of the CSV and extracts only the 'sentence' and 'expected_sentiment' columns.\n",
    "        \"\"\"\n",
    "        # .iterrows() loops through the rows geeksforgeeks\n",
    "        for _, row in self.df.iterrows():\n",
    "            positive_sentence = row[\"SAMPLE_POSITIVE_ANSWERS\"]\n",
    "            positive_sentiment_expected = row[\"POSITIVE_SENTIMENT\"]\n",
    "            \n",
    "            negative_sentence = row[\"SAMPLE_NEGATIVE_ANSWERS\"]\n",
    "            negative_sentiment_expected = row[\"NEGATIVE_SENTIMENT\"]\n",
    "            \n",
    "            self.positive_sentences.append((positive_sentence, positive_sentiment_expected))\n",
    "            self.negative_sentences.append((negative_sentence, negative_sentiment_expected))\n",
    "           \n",
    "        # temp will store both positive and negative sentiment sentences to return a singular array \n",
    "        # containing both \n",
    "        temp = []\n",
    "        for entry in self.positive_sentences: \n",
    "            temp.append(entry)\n",
    "            \n",
    "        for entry in self.negative_sentences:\n",
    "            temp.append(entry)\n",
    "        return temp\n",
    "\n",
    "    \"\"\"\n",
    "    might not be needed since we can already grab it using a single function\n",
    "    we could process the sentiment values here (int values)\n",
    "    or just return a list of expected_sentiments\n",
    "    \"\"\"\n",
    "    # def grab_sentiment(self):\n",
    "    #     return self.df[\"expected_sentiment\"].tolist()\n",
    "    \n",
    "# sample main \n",
    "if __name__ == \"__main__\":\n",
    "    # testing the csvReadInFunctions\n",
    "    reader = csv_read_in_functions(\"../dataset/hirevue-answer-sheet.csv\")\n",
    "    sentence_pairs = reader.grab_sentences_and_sentiment()\n",
    "    \n",
    "    for sentence, sentiment in sentence_pairs:\n",
    "        print(f\"Sentence: {sentence}\\nExpected Sentiment: {sentiment}\\n\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "# upgrades accelerate to at least 0.26.0\n",
    "%pip install \"accelerate>=0.26.0\"\n",
    "# installs huggingface_hub as needed \n",
    "%pip install huggingface_hub\n",
    "# installs pytorch as needed\n",
    "\n",
    "# ensure dependencies are installed properly\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import accelerate\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "# should reload kernel as needed\n",
    "# stackoverflow article: https://stackoverflow.com/questions/63595912/how-to-restart-kernel-in-vscode-to-completely-reload-imports\n",
    "\n",
    "# get the CSV reader and grab the data from LatinCsvReadInFunctions class\n",
    "csv_reader = csv_read_in_functions(\"../dataset/hirevue-answer-sheet.csv\")\n",
    "sentences_data = csv_reader.grab_sentences_and_sentiment()\n",
    "\n",
    "class sentiment_analysis:\n",
    "    def __init__(self, sentences_data, model_name=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "        csv_reader = csv_read_in_functions(\"../dataset/hirevue-answer-sheet.csv\")\n",
    "        sentences_data = csv_reader.grab_sentences_and_sentiment()\n",
    "        \n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "        # use_fast here since the tokenizer.json file doesn't exist in the bert model and we need to rely on vocab.json and merges.txt\n",
    "        \n",
    "        self.sentences_data = sentences_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        \n",
    "        # initializes MPS and ensures the model is set to the correct device before any training starts\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "        \n",
    "        # mapping sentiment to numeric values (may need to adjust as needed later depending on results)\n",
    "        self.label2id = {\"positive\": 0, \"negative\": 1}\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(self.label2id)\n",
    "        )\n",
    "        \n",
    "    def prepare_dataset(self, max_length=128):\n",
    "        \"\"\"\n",
    "        convert the sentences_data into a format suitable for training\n",
    "        we're using a custom pytorch format here\n",
    "        \"\"\"\n",
    "        class sentiment_dataset(Dataset):\n",
    "            def __init__(self, data, tokenizer, max_length):\n",
    "                self.data = data\n",
    "                self.tokenizer = tokenizer\n",
    "                self.max_length = max_length   # ensures max length of sentiment doesn't exceed this\n",
    "            \n",
    "            def __len__(self):\n",
    "                return len(self.data)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                sentence, sentiment = self.data[idx]\n",
    "                # vonvert sentiment to label; \n",
    "                # assuming self.label2id is available in the outer scope\n",
    "                \n",
    "                if sentiment.lower() == \"positive\":\n",
    "                    label = 0 \n",
    "                else:\n",
    "                    label = 1  \n",
    "                \n",
    "                encoding = self.tokenizer(\n",
    "                    sentence,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                # got from hugging face documentation\n",
    "                encoding = {key: value.squeeze(0) for key, value in encoding.items()}\n",
    "                encoding[\"labels\"] = torch.tensor(label)\n",
    "                return encoding\n",
    "        \n",
    "        return sentiment_dataset(self.sentences_data, self.tokenizer, max_length)\n",
    "    \n",
    "    # training, evaluation, and prediction methods here.\n",
    "    def train(self, dataset, output_dir=\"./results\", num_train_epochs=3, per_device_train_batch_size=16):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            logging_steps=10,\n",
    "            save_steps=500,\n",
    "            evaluation_strategy=\"no\",\n",
    "            # Note: don't use evaluation_strategy will be deprecated in version 4.46 transformers \n",
    "            # use eval_strategy \n",
    "            no_cuda=True \n",
    "            # this applies for macbook use only, if you're on windows or a linux system, comment no_cuda out \n",
    "            # this avoids the \"RuntimeError: Placeholder storage has not been allocated on MPS device!\" error\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "# testing\n",
    "lsa = sentiment_analysis(sentences_data)\n",
    "dataset = lsa.prepare_dataset()\n",
    "lsa.train(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample main \n",
    "if __name__ == \"__main__\":\n",
    "    # testing the csvReadInFunctions\n",
    "    reader = csv_read_in_functions(\"../dataset/hirevue-answer-sheet.csv\")\n",
    "    sentence_pairs = reader.grab_sentences_and_sentiment()\n",
    "    \n",
    "    for sentence, sentiment in sentence_pairs:\n",
    "        print(f\"Sentence: {sentence}\\nExpected Sentiment: {sentiment}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
